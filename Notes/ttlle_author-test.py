# -*- coding: utf-8 -*-
"""
Created on Sun Apr 7 19:14:19 2024

@author: Shin Jeong-yoon
"""
import pandas as pd 
from selenium import webdriver 
from selenium .webdriver .chrome .options import Options 
from bs4 import BeautifulSoup 
import time 

def parser (url ):
    """Parse the given URL and return the BeautifulSoup object."""
    options =Options ()
    options .headless =False # Show browser GUI (for debugging)
    options .add_argument ("start-maximized")# Maximize browser window
    driver =webdriver .Chrome (options =options )
    driver .get (url )

    # Scroll down to load the page
    screen_height =driver .execute_script ("return window.screen.height;")
    for _ in range (8 ):# Scroll down 8 times
        driver .execute_script ("window.scrollTo(0, document.body.scrollHeight);")
        time .sleep (3 )

        # Get the page source
    html_text =driver .page_source 
    driver .quit ()

    # Parse HTML using BeautifulSoup
    soup =BeautifulSoup (html_text ,'html.parser')
    print (f'Scraping: {soup .title .text }')
    return soup 

def extract_product_data (soup ):
    """Extract product (book) data from the parsed HTML soup."""
    product_data =[]

    for product in soup .find_all ("div",attrs ={'id':'gridItemRoot','class':'a-column a-span12 a-text-center _p13n-zg-list-grid-desktop_style_grid-column__2hIsc'}):
        name_elem =product .find ("div",attrs ={"class":"_p13n-zg-list-grid-desktop_truncationStyles_p13n-sc-css-line-clamp-1__1Fn1y"})
        author_elem =product .find ("div",attrs ={"class":"a-row a-size-small"})

        if name_elem and author_elem :
            product_data .append ({
            'Product':name_elem .text .strip (),
            'Author':author_elem .text .strip ()
            })

    return pd .DataFrame (product_data )

def main ():
# Main bestseller page URLs
    main_url ='https://www.amazon.com/gp/bestsellers/2023/books'
    page2_url ='https://www.amazon.com/gp/bestsellers/2023/books/ref=zg_bs_pg_2/139-1234567-1234567?ie=UTF8&pg=2'

    # Parse main and page 2 URLs
    main_soup =parser (main_url )
    page2_soup =parser (page2_url )

    # Extract product data from main and page 2 soups
    df_main =extract_product_data (main_soup )
    df_page2 =extract_product_data (page2_soup )

    # Concatenate data frames
    df_all =pd .concat ([df_main ,df_page2 ],ignore_index =True )

    # Save to CSV
    df_all .to_csv ('Amazon_Bestseller_books.csv',index =False )

if __name__ =='__main__':
    main ()